{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83932e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: run for dependencies\n",
    "# %pip install --upgrade pip\n",
    "# %pip install \"ultralytics>=8.3.0\" opencv-python pillow scikit-learn albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd48c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: imports + hard fences\n",
    "import os, shutil, random\n",
    "from pathlib import Path\n",
    "import cv2, numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "ROOT = PROJECT_ROOT\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"MLFLOW_DISABLE_ENV\"] = \"true\"\n",
    "os.environ[\"ULTRALYTICS_SETTINGS_RESET\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02063c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Capacitor', 'IC', 'Processor', 'Tan_Cap']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: paths + params\n",
    "RAW_DATA  = ROOT / \"data\"\n",
    "YOLO_ROOT = ROOT / \"yolo_data\"\n",
    "RUNS_DIR  = YOLO_ROOT / \"runs\"\n",
    "\n",
    "IM_TRAIN = YOLO_ROOT / \"images\" / \"train\"\n",
    "IM_VAL   = YOLO_ROOT / \"images\" / \"val\"\n",
    "LB_TRAIN = YOLO_ROOT / \"labels\" / \"train\"\n",
    "LB_VAL   = YOLO_ROOT / \"labels\" / \"val\"\n",
    "YOLO_YAML = YOLO_ROOT / \"data.yaml\"\n",
    "\n",
    "VAL_SPLIT = 0.15\n",
    "AUG_MULTIPLIER = 5\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED)\n",
    "\n",
    "CENTER_BOX_FRAC = 0.30\n",
    "\n",
    "MODEL_NAME = \"yolov8n.pt\"\n",
    "IMG_SIZE   = 416\n",
    "EPOCHS     = 40\n",
    "BATCH      = 2\n",
    "WORKERS    = 0\n",
    "\n",
    "assert RAW_DATA.exists(), f\"Missing: {RAW_DATA}\"\n",
    "class_names = sorted([d.name for d in RAW_DATA.iterdir() if d.is_dir()])\n",
    "assert len(class_names) == 4, f\"Expected 4 classes, got {len(class_names)}: {class_names}\"\n",
    "class_to_idx = {c:i for i,c in enumerate(class_names)}\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "VALID_EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a339154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: io helpers\n",
    "def list_images():\n",
    "    items = []\n",
    "    for cls in class_names:\n",
    "        for p in (RAW_DATA/cls).rglob(\"*\"):\n",
    "            if p.suffix.lower() in VALID_EXTS:\n",
    "                items.append((p, class_to_idx[cls]))\n",
    "    return items\n",
    "\n",
    "def load_rgb(path):\n",
    "    try:\n",
    "        im = Image.open(path).convert(\"RGB\")\n",
    "        return np.array(im)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {path} ({e})\"); return None\n",
    "\n",
    "def find_largest_box(img):\n",
    "    h, w = img.shape[:2]\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    th = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                               cv2.THRESH_BINARY_INV, 31, 5)\n",
    "    k = np.ones((3,3), np.uint8)\n",
    "    th = cv2.morphologyEx(th, cv2.MORPH_OPEN, k, iterations=1)\n",
    "    th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, k, iterations=2)\n",
    "    cnts, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not cnts: return None\n",
    "    area_min = 0.02 * (w*h)\n",
    "    cands = []\n",
    "    for c in cnts:\n",
    "        x,y,ww,hh = cv2.boundingRect(c)\n",
    "        area = ww*hh\n",
    "        if area < area_min: continue\n",
    "        ar = ww / max(1, hh)\n",
    "        if ar < 0.2 or ar > 5.0: continue\n",
    "        cands.append((x,y,x+ww,y+hh, area))\n",
    "    if not cands: return None\n",
    "    x1,y1,x2,y2,_ = max(cands, key=lambda t:t[4])\n",
    "    px, py = int(0.02*w), int(0.02*h)\n",
    "    x1 = max(0, x1-px); y1 = max(0, y1-py)\n",
    "    x2 = min(w-1, x2+px); y2 = min(h-1, y2+py)\n",
    "    return (x1,y1,x2,y2)\n",
    "\n",
    "def yolo_line_from_box(cls_id, box, w, h):\n",
    "    x1,y1,x2,y2 = box\n",
    "    bw, bh = (x2-x1), (y2-y1)\n",
    "    cx, cy = x1 + bw/2, y1 + bh/2\n",
    "    return f\"{cls_id} {cx/w:.6f} {cy/h:.6f} {bw/w:.6f} {bh/h:.6f}\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d929128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 414 images\n",
      "Train: 351 | Val: 63\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: split + dirs\n",
    "all_items = list_images()\n",
    "assert len(all_items) > 0, \"No images found under ./data/<class>.\"\n",
    "print(f\"Found {len(all_items)} images\")\n",
    "\n",
    "y = [cid for _, cid in all_items]\n",
    "idx = np.arange(len(all_items))\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SPLIT, random_state=RANDOM_SEED)\n",
    "train_idx, val_idx = next(sss.split(idx, y))\n",
    "train_items = [all_items[i] for i in train_idx]\n",
    "val_items   = [all_items[i] for i in val_idx]\n",
    "print(f\"Train: {len(train_items)} | Val: {len(val_items)}\")\n",
    "\n",
    "# SAFE: keep runs/ (weights) intact; only reset dataset folders\n",
    "for p in [IM_TRAIN, IM_VAL, LB_TRAIN, LB_VAL]:\n",
    "    if p.parent.parent.exists() and p.parent.parent != YOLO_ROOT:\n",
    "        pass  # just being defensive\n",
    "# recreate images/labels trees\n",
    "for p in [IM_TRAIN, IM_VAL, LB_TRAIN, LB_VAL]:\n",
    "    if p.exists():\n",
    "        shutil.rmtree(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)  # keep existing runs\n",
    "\n",
    "IM_TRAIN.mkdir(parents=True, exist_ok=True); IM_VAL.mkdir(parents=True, exist_ok=True)\n",
    "LB_TRAIN.mkdir(parents=True, exist_ok=True); LB_VAL.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c48182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\armaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\PIL\\Image.py:3570: UserWarning: image file could not be identified because AVIF support not installed\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu_253658-1870 (3).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu_253658-1870 (3).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-microprocessor-cpu-close-up-isolated-white-backgrou.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-microprocessor-cpu-close-up-isolated-white-backgrou.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\single-microchip-isolated-white-background_941600-88404(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\single-microchip-isolated-white-background_941600-88404(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\high-angle-view-computer-chip-against-white-background_10489 (1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\high-angle-view-computer-chip-against-white-background_10489 (1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\single-microchip-isolated-white-background_941600-88404.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\single-microchip-isolated-white-background_941600-88404.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-microprocessor-cpu-close-up-isolated-white-backgrou(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-microprocessor-cpu-close-up-isolated-white-backgrou(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\central-processing-unit-computer-chip_41929-1449.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\central-processing-unit-computer-chip_41929-1449.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\directly-computer-chip-against-white-background_1048944-2255(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\directly-computer-chip-against-white-background_1048944-2255(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu-central-processing-unit-microchip-isolated-white-backgro.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu-central-processing-unit-microchip-isolated-white-backgro.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-processor-cpu-white-background_627429-130.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-processor-cpu-white-background_627429-130.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-cpu-png-mockup-closeup_53876-1021502 (1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-cpu-png-mockup-closeup_53876-1021502 (1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\directly-computer-chip-against-white-background_1048944-2255.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\directly-computer-chip-against-white-background_1048944-2255.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-processors-cpu_1155277-2155.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-processors-cpu_1155277-2155.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-microprocessor-cpu-close-up-isolated-white-backgrou (3).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-microprocessor-cpu-close-up-isolated-white-backgrou (3).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu_253658-1870.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu_253658-1870.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\central-processor-unit-cpu-chip-blue-monochrome-toned-white-.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\central-processor-unit-cpu-chip-blue-monochrome-toned-white-.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu-processor-chip-isolated-white-background_1048944-1362788.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu-processor-chip-isolated-white-background_1048944-1362788.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\high-angle-view-information-sign-against-white-background_10.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\high-angle-view-information-sign-against-white-background_10.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-cpu-png-mockup-closeup_53876-1021502(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-cpu-png-mockup-closeup_53876-1021502(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu_253658-720(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu_253658-720(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu-central-processor-unit-isolated-background_77829-3621.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu-central-processor-unit-isolated-background_77829-3621.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-processor-cpu-central-processing-unit-microchip-iso (2).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-processor-cpu-central-processing-unit-microchip-iso (2).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\lowpower-microprocessor-portable-computers-isolated_1120557-(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\lowpower-microprocessor-portable-computers-isolated_1120557-(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\central-computer-processors-cpu-high-resolution-3d-render-wh (1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\central-computer-processors-cpu-high-resolution-3d-render-wh (1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\close-up-yellow-pencil-white-background_1048944-22361352.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\close-up-yellow-pencil-white-background_1048944-22361352.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\lowpower-microprocessor-portable-computers-isolated_1120557-.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\lowpower-microprocessor-portable-computers-isolated_1120557-.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\high-angle-view-computer-chip-against-white-background_10489(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\high-angle-view-computer-chip-against-white-background_10489(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-processor_769373-2529.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-processor_769373-2529.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu-central-processor-unit-isolated_77829-4075.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu-central-processor-unit-isolated_77829-4075.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\intel-processor-shines-sleek-white-background-captivating-ph.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\intel-processor-shines-sleek-white-background-captivating-ph.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu_253658-1870(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu_253658-1870(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\high-angle-view-information-sign-against-white-background_10(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\high-angle-view-information-sign-against-white-background_10(1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu_253658-720 (1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu_253658-720 (1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-engineering-microprocessor-processor-isolated-white.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-engineering-microprocessor-processor-isolated-white.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu-central-processor-unit-isolated_77829-4020.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu-central-processor-unit-isolated_77829-4020.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-cpu-processor-closeup-visible-gold-pins-isolated-wh.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-cpu-processor-closeup-visible-gold-pins-isolated-wh.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu-processor-chip-isolated-white-background_1048944-2976349.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu-processor-chip-isolated-white-background_1048944-2976349.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\close-up-yellow-pencil-white-background_1048944-22361352(1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\close-up-yellow-pencil-white-background_1048944-22361352(1).jpg')\n",
      "Wrote 313 -> d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\images\\train\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-micro-processor_769373-2527.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-micro-processor_769373-2527.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\electronic-collection-computer-processor-from-top-side-isola.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\electronic-collection-computer-processor-from-top-side-isola.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-processor-cpu-central-processing-unit-microchip-iso.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-processor-cpu-central-processing-unit-microchip-iso.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu-central-processor-unit-isolated_77829-4729.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu-central-processor-unit-isolated_77829-4729.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\high-angle-view-computer-chip-against-white-background_10489.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\high-angle-view-computer-chip-against-white-background_10489.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\high-angle-view-information-sign-against-white-background_10 (1).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\high-angle-view-information-sign-against-white-background_10 (1).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-cpu-png-mockup-closeup_53876-1021502.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-cpu-png-mockup-closeup_53876-1021502.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\close-up-yellow-pencil-white-background_1048944-22361352 (3).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\close-up-yellow-pencil-white-background_1048944-22361352 (3).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\directly-computer-chip-against-white-background_1048944-2255 (5).jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\directly-computer-chip-against-white-background_1048944-2255 (5).jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\computer-microchip-electronic-microcircuit_99433-5234.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\computer-microchip-electronic-microcircuit_99433-5234.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu-central-processor-unit_996086-23712.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu-central-processor-unit_996086-23712.jpg')\n",
      "[skip] d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\data\\Processor\\cpu_253658-720.jpg (cannot identify image file 'd:\\\\Coding\\\\Projects\\\\Classifier\\\\Waste_Classifier\\\\data\\\\Processor\\\\cpu_253658-720.jpg')\n",
      "Wrote 51 -> d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\images\\val\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: write base split\n",
    "def write_split(items, im_dst, lb_dst):\n",
    "    kept = 0\n",
    "    for src_path, cls_id in items:\n",
    "        img = load_rgb(src_path)\n",
    "        if img is None: continue\n",
    "        h, w = img.shape[:2]\n",
    "        box = find_largest_box(img)\n",
    "        if box is None:\n",
    "            bw, bh = int(0.5*w), int(0.5*h)\n",
    "            x1 = (w-bw)//2; y1 = (h-bh)//2\n",
    "            box = (x1,y1,x1+bw,y1+bh)\n",
    "        out_img = im_dst / src_path.name\n",
    "        Image.fromarray(img).save(out_img)\n",
    "        yolo_line = yolo_line_from_box(cls_id, box, w, h)\n",
    "        (lb_dst / (out_img.stem + \".txt\")).write_text(yolo_line)\n",
    "        kept += 1\n",
    "    print(f\"Wrote {kept} -> {im_dst}\")\n",
    "\n",
    "write_split(train_items, IM_TRAIN, LB_TRAIN)\n",
    "write_split(val_items,   IM_VAL,   LB_VAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7104c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\armaa\\AppData\\Local\\Temp\\ipykernel_3796\\732167412.py:6: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(5.0, 25.0), p=0.2),\n",
      "C:\\Users\\armaa\\AppData\\Local\\Temp\\ipykernel_3796\\732167412.py:8: UserWarning: Argument(s) 'mode' are not valid for transform Affine\n",
      "  A.Affine(scale=(0.85, 1.15), translate_percent=(0.0, 0.05), rotate=(-10, 10),\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: aug pipeline + helpers\n",
    "AUG_PIPE = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.6),\n",
    "    A.HueSaturationValue(hue_shift_limit=8, sat_shift_limit=20, val_shift_limit=15, p=0.4),\n",
    "    A.GaussNoise(var_limit=(5.0, 25.0), p=0.2),\n",
    "    A.MotionBlur(blur_limit=5, p=0.2),\n",
    "    A.Affine(scale=(0.85, 1.15), translate_percent=(0.0, 0.05), rotate=(-10, 10),\n",
    "             shear=(-5, 5), mode=cv2.BORDER_REFLECT_101, p=0.5),\n",
    "    A.Resize(height=int(IMG_SIZE), width=int(IMG_SIZE), interpolation=cv2.INTER_LINEAR),\n",
    "], bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"], min_visibility=0.4))\n",
    "\n",
    "def yolo_to_voc_px(cx, cy, bw, bh, w, h):\n",
    "    x1 = (cx - bw/2) * w; y1 = (cy - bh/2) * h\n",
    "    x2 = (cx + bw/2) * w; y2 = (cy + bh/2) * h\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "def voc_px_to_yolo(x1, y1, x2, y2, w, h):\n",
    "    bw = max(1.0, x2 - x1); bh = max(1.0, y2 - y1)\n",
    "    cx = x1 + bw/2; cy = y1 + bh/2\n",
    "    return [cx / w, cy / h, bw / w, bh / h]\n",
    "\n",
    "def clamp_voc(x1, y1, x2, y2, w, h):\n",
    "    x1 = max(0.0, min(float(w - 1), x1))\n",
    "    y1 = max(0.0, min(float(h - 1), y1))\n",
    "    x2 = max(0.0, min(float(w - 1), x2))\n",
    "    y2 = max(0.0, min(float(h - 1), y2))\n",
    "    if x2 <= x1: x2 = min(w - 1.0, x1 + 1.0)\n",
    "    if y2 <= y1: y2 = min(h - 1.0, y1 + 1.0)\n",
    "    return [x1, y1, x2, y2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c930e4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented images written: 1540\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: augment\n",
    "def augment_train_set(mult=AUG_MULTIPLIER):\n",
    "    ims = sorted([p for p in IM_TRAIN.iterdir() if p.suffix.lower() in VALID_EXTS])\n",
    "    count = 0\n",
    "    for p in ims:\n",
    "        lbp = LB_TRAIN / (p.stem + \".txt\")\n",
    "        if not lbp.exists(): continue\n",
    "        img_bgr = cv2.imread(str(p))\n",
    "        if img_bgr is None: continue\n",
    "        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        lines = [ln.strip().split() for ln in lbp.read_text().strip().splitlines() if ln.strip()]\n",
    "        if not lines: continue\n",
    "        bboxes_voc, class_labels = [], []\n",
    "        for ln in lines:\n",
    "            c = int(ln[0]); cx, cy, bw, bh = map(float, ln[1:5])\n",
    "            x1, y1, x2, y2 = yolo_to_voc_px(cx, cy, bw, bh, w, h)\n",
    "            bboxes_voc.append([x1, y1, x2, y2]); class_labels.append(c)\n",
    "\n",
    "        for k in range(mult):\n",
    "            try:\n",
    "                t = AUG_PIPE(image=img, bboxes=bboxes_voc, class_labels=class_labels)\n",
    "            except Exception:\n",
    "                continue\n",
    "            im2 = t[\"image\"]; bb2 = t[\"bboxes\"]; cl2 = t[\"class_labels\"]\n",
    "            if len(bb2) == 0: continue\n",
    "            h2, w2 = im2.shape[:2]\n",
    "            filtered = []\n",
    "            for (x1, y1, x2, y2), c in zip(bb2, cl2):\n",
    "                x1, y1, x2, y2 = clamp_voc(x1, y1, x2, y2, w2, h2)\n",
    "                bw = x2 - x1; bh = y2 - y1\n",
    "                if bw * bh < 4: continue\n",
    "                filtered.append((c, x1, y1, x2, y2))\n",
    "            if not filtered: continue\n",
    "\n",
    "            out_img = IM_TRAIN / f\"{p.stem}_aug{k}.jpg\"\n",
    "            cv2.imwrite(str(out_img), cv2.cvtColor(im2, cv2.COLOR_RGB2BGR))\n",
    "            with open(LB_TRAIN / f\"{p.stem}_aug{k}.txt\", \"w\") as f:\n",
    "                for c, x1, y1, x2, y2 in filtered:\n",
    "                    cx, cy, bw, bh = voc_px_to_yolo(x1, y1, x2, y2, w2, h2)\n",
    "                    cx = min(1.0, max(0.0, cx)); cy = min(1.0, max(0.0, cy))\n",
    "                    bw = min(1.0, max(1e-6, bw)); bh = min(1.0, max(1e-6, bh))\n",
    "                    f.write(f\"{c} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
    "            count += 1\n",
    "    print(f\"Augmented images written: {count}\")\n",
    "\n",
    "augment_train_set(mult=AUG_MULTIPLIER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f52ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\n",
      "train: images/train\n",
      "val: images/val\n",
      "names:\n",
      "  0: Capacitor\n",
      "  1: IC\n",
      "  2: Processor\n",
      "  3: Tan_Cap\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: data.yaml\n",
    "YOLO_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "with open(YOLO_YAML, \"w\") as f:\n",
    "    f.write(f\"path: {str(YOLO_ROOT)}\\n\")\n",
    "    f.write(\"train: images/train\\n\")\n",
    "    f.write(\"val: images/val\\n\")\n",
    "    f.write(\"names:\\n\")\n",
    "    for i,n in enumerate(class_names):\n",
    "        f.write(f\"  {i}: {n}\\n\")\n",
    "print(YOLO_YAML.read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cd55dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: train\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def train_yolo():\n",
    "    model = YOLO(MODEL_NAME)\n",
    "    run_name = \"train_exp\"\n",
    "    model.train(\n",
    "        data=str(YOLO_YAML),\n",
    "        device=\"cpu\",\n",
    "        imgsz=IMG_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        batch=BATCH,\n",
    "        workers=WORKERS,\n",
    "        amp=False,\n",
    "        mosaic=0.0,\n",
    "        cache=False,\n",
    "        freeze=0,\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        project=str(RUNS_DIR),\n",
    "        name=run_name,\n",
    "        exist_ok=True,\n",
    "        save=True,\n",
    "        save_period=-1,\n",
    "        resume=False,\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "    best_path = Path(getattr(model.trainer, \"best\", \"\")).resolve()\n",
    "    assert best_path.is_file(), f\"Best weights not found: {best_path}\"\n",
    "    assert RUNS_DIR.resolve() in best_path.parents, f\"Outside project: {best_path}\"\n",
    "    print(\"Best model:\", best_path)\n",
    "    return best_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ef48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: validate\n",
    "def validate(weights_path: Path):\n",
    "    model = YOLO(str(weights_path))\n",
    "    run_name = \"val_exp\"\n",
    "    val_res = model.val(\n",
    "        data=str(YOLO_YAML),\n",
    "        imgsz=IMG_SIZE,\n",
    "        device=\"cpu\",\n",
    "        project=str(RUNS_DIR),\n",
    "        name=run_name,\n",
    "        exist_ok=True,\n",
    "        save=False,\n",
    "        plots=False\n",
    "    )\n",
    "    print(f\"mAP50-95: {val_res.box.map:.4f} | mAP50: {val_res.box.map50:.4f}\")\n",
    "    for i, n in enumerate(class_names):\n",
    "        ap = val_res.box.maps[i] if i < len(val_res.box.maps) else float(\"nan\")\n",
    "        print(f\"{n}: AP50-95={ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ac5bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: helpers\n",
    "def is_centered(box_xyxy, frame_w, frame_h, frac=CENTER_BOX_FRAC):\n",
    "    x1,y1,x2,y2 = box_xyxy\n",
    "    cx = 0.5*(x1+x2); cy = 0.5*(y1+y2)\n",
    "    fx = frame_w*0.5; fy = frame_h*0.5\n",
    "    hw = frame_w*frac*0.5; hh = frame_h*frac*0.5\n",
    "    return (fx-hw) <= cx <= (fx+hw) and (fy-hh) <= cy <= (fy+hh)\n",
    "\n",
    "def load_weights_or_fail(path: Path) -> Path:\n",
    "    path = Path(path).resolve()\n",
    "    assert path.is_file(), f\"Missing weights: {path}\"\n",
    "    assert RUNS_DIR.resolve() in path.parents, f\"Refusing non-project weights: {path}\"\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e6d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: predict image\n",
    "def predict_image(image_path, weights_path, conf_th=0.10):\n",
    "    weights_path = load_weights_or_fail(weights_path)\n",
    "    mdl = YOLO(str(weights_path))\n",
    "    res = mdl.predict(\n",
    "        source=str(image_path),\n",
    "        imgsz=IMG_SIZE,\n",
    "        conf=conf_th,\n",
    "        verbose=False,\n",
    "        device=\"cpu\",\n",
    "        save=False, save_txt=False, save_conf=False,\n",
    "        project=str(RUNS_DIR), name=\"predict_exp\", exist_ok=True\n",
    "    )\n",
    "    for r in res:\n",
    "        if r.boxes is None or len(r.boxes)==0:\n",
    "            print(\"No detections.\"); continue\n",
    "        for b in r.boxes:\n",
    "            cid = int(b.cls.item()); conf = float(b.conf.item())\n",
    "            xyxy = list(map(int, b.xyxy.squeeze().cpu().numpy().tolist()))\n",
    "            print(f\"{class_names[cid]} @ {conf:.2f} -> {xyxy}\")\n",
    "    for r in res:\n",
    "        r.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847601ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: live webcam\n",
    "def live_video_classify(weights_path, cam_index=0, conf_th=0.15):\n",
    "    from collections import deque\n",
    "    weights_path = load_weights_or_fail(weights_path)\n",
    "    mdl = YOLO(str(weights_path))\n",
    "    cap = cv2.VideoCapture(cam_index)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Could not open webcam.\"); return\n",
    "    history = deque(maxlen=5)\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "            h, w = frame.shape[:2]\n",
    "            res = mdl.predict(\n",
    "                source=frame,\n",
    "                imgsz=IMG_SIZE,\n",
    "                conf=conf_th,\n",
    "                verbose=False,\n",
    "                device=\"cpu\",\n",
    "                save=False, save_txt=False, save_conf=False,\n",
    "                project=str(RUNS_DIR), name=\"live_exp\", exist_ok=True\n",
    "            )\n",
    "            preds = []\n",
    "            for r in res:\n",
    "                if r.boxes is None: continue\n",
    "                for b in r.boxes:\n",
    "                    cid = int(b.cls.item()); conf = float(b.conf.item())\n",
    "                    xyxy = b.xyxy.squeeze().cpu().numpy().tolist()\n",
    "                    preds.append((cid, conf, xyxy))\n",
    "\n",
    "            fx, fy = int(w*0.5), int(h*0.5)\n",
    "            hw, hh = int(w*CENTER_BOX_FRAC*0.5), int(h*CENTER_BOX_FRAC*0.5)\n",
    "            cv2.rectangle(frame, (fx-hw, fy-hh), (fx+hw, fy+hh), (128,128,128), 1)\n",
    "\n",
    "            any_yes = False\n",
    "            for (cid, conf, xyxy) in preds:\n",
    "                x1,y1,x2,y2 = map(int, xyxy)\n",
    "                centered = is_centered(xyxy, w, h, frac=CENTER_BOX_FRAC)\n",
    "                any_yes |= centered\n",
    "                color = (0,255,0) if centered else (0,165,255)\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), color, 2)\n",
    "                label = f\"{class_names[cid]} {conf:.2f}\"\n",
    "                cv2.putText(frame, label, (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "            history.append(1 if any_yes else 0)\n",
    "            stable_yes = sum(history) >= 3\n",
    "            status = \"YES\" if stable_yes else \"NO\"\n",
    "            cv2.putText(frame, status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
    "                        (0,255,0) if stable_yes else (0,0,255), 2)\n",
    "\n",
    "            cv2.imshow(\"Live Detection (q to quit)\", frame)\n",
    "            if (cv2.waitKey(1) & 0xFF) == ord('q'): break\n",
    "    finally:\n",
    "        cap.release(); cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1b28d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.222 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.210  Python-3.13.9 torch-2.8.0+cu126 CPU (Intel Core i9-14900HX)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=2, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=0, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=train_exp, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=d:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs\\train_exp, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 2.91.5 MB/s, size: 39.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\labels\\train... 1849 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1849/1849 500.3it/s 3.7s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\labels\\train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 9.29.1 MB/s, size: 135.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\labels\\val.cache... 51 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 51/51 106.8Kit/s 0.0s\n",
      "Plotting labels to D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs\\train_exp\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mD:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs\\train_exp\u001b[0m\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/40         0G      1.146      2.444      1.503          1        416: 100% ━━━━━━━━━━━━ 925/925 4.9it/s 3:08<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.7it/s 2.3s0.2s\n",
      "                   all         51         51       0.76       0.57      0.594      0.454\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/40         0G      1.145      1.902      1.466          1        416: 100% ━━━━━━━━━━━━ 925/925 3.9it/s 3:55<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 4.8it/s 2.7s0.2s\n",
      "                   all         51         51      0.506      0.553      0.549      0.398\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/40         0G      1.147      1.645      1.458          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.541      0.679      0.646       0.43\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/40         0G      1.127      1.469      1.419          1        416: 100% ━━━━━━━━━━━━ 925/925 3.4it/s 4:28<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.1it/s 2.6s0.2s\n",
      "                   all         51         51      0.604      0.768      0.665      0.486\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/40         0G      1.082      1.255      1.384          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:28<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.4s0.2s\n",
      "                   all         51         51      0.654      0.607      0.597      0.443\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/40         0G      1.023      1.175      1.344          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.5it/s 2.4s0.2s\n",
      "                   all         51         51      0.482      0.654      0.589      0.417\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/40         0G      1.022      1.094      1.339          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:21<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.4s0.2s\n",
      "                   all         51         51      0.778      0.687      0.802      0.574\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/40         0G     0.9656     0.9976      1.287          1        416: 100% ━━━━━━━━━━━━ 925/925 3.6it/s 4:20<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.4s0.2s\n",
      "                   all         51         51      0.641      0.814      0.774      0.545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/40         0G     0.9289     0.9196       1.27          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.2it/s 2.5s0.2s\n",
      "                   all         51         51      0.633      0.709      0.721      0.529\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/40         0G      0.893     0.8787      1.251          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:27<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.5s0.2s\n",
      "                   all         51         51       0.61      0.747      0.736      0.551\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/40         0G     0.8699     0.8182      1.216          1        416: 100% ━━━━━━━━━━━━ 925/925 3.4it/s 4:30<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.1it/s 2.5s0.2s\n",
      "                   all         51         51      0.716      0.803      0.763      0.609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/40         0G     0.8378     0.8092      1.192          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.697      0.697      0.706      0.519\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/40         0G     0.8317      0.754      1.208          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:22<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.759      0.777      0.789      0.607\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/40         0G     0.8139     0.7301      1.187          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.659      0.753      0.783      0.633\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/40         0G     0.7825     0.6985      1.164          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:25<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.1it/s 2.5s0.2s\n",
      "                   all         51         51      0.639      0.725      0.698      0.587\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/40         0G     0.7514     0.6603      1.149          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:22<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.5s0.2s\n",
      "                   all         51         51      0.675      0.774      0.735      0.602\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/40         0G     0.7356     0.6501      1.136          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.7it/s 2.3s0.2s\n",
      "                   all         51         51      0.799      0.803      0.826      0.625\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/40         0G     0.7083     0.6172       1.11          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:27<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.4s0.2s\n",
      "                   all         51         51      0.722      0.785      0.778       0.62\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/40         0G     0.7118     0.6062      1.112          1        416: 100% ━━━━━━━━━━━━ 925/925 3.6it/s 4:20<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.7it/s 2.3s0.2s\n",
      "                   all         51         51      0.874      0.796      0.822      0.633\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/40         0G     0.7022     0.5699      1.092          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.2it/s 2.5s0.2s\n",
      "                   all         51         51      0.695      0.814       0.78      0.633\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/40         0G       0.66     0.5365      1.082          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.2it/s 2.5s0.2s\n",
      "                   all         51         51      0.775      0.777       0.78      0.673\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/40         0G     0.6525     0.5319      1.078          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 4.5it/s 2.9s0.2s\n",
      "                   all         51         51      0.764      0.776      0.802      0.654\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/40         0G     0.6378     0.5268      1.067          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:27<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.674      0.902      0.827      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/40         0G     0.6369     0.5106      1.055          1        416: 100% ━━━━━━━━━━━━ 925/925 3.4it/s 4:28<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.1it/s 2.6s0.2s\n",
      "                   all         51         51      0.704      0.806      0.801      0.646\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/40         0G     0.6149     0.5008       1.05          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:23<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.5it/s 2.4s0.2s\n",
      "                   all         51         51      0.772      0.779      0.836      0.663\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/40         0G     0.5979     0.4767      1.046          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.6it/s 2.3s0.2s\n",
      "                   all         51         51      0.807      0.811      0.825       0.67\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/40         0G     0.5804     0.4568      1.027          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:27<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.5s0.2s\n",
      "                   all         51         51      0.847      0.784      0.857      0.679\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/40         0G     0.5662     0.4448      1.024          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:22<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.719      0.756      0.779       0.65\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/40         0G      0.559     0.4444      1.026          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:25<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.7it/s 2.3s0.2s\n",
      "                   all         51         51      0.739      0.839      0.765      0.645\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/40         0G     0.5606     0.4206      1.017          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:24<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 4.8it/s 2.7s0.2s\n",
      "                   all         51         51      0.793      0.867      0.829      0.707\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      31/40         0G     0.5409     0.4095     0.9999          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:23<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.798      0.802      0.843      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      32/40         0G     0.5368     0.4159     0.9992          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:21<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.5it/s 2.4s0.2s\n",
      "                   all         51         51      0.816      0.804      0.819      0.665\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      33/40         0G     0.5169     0.3998     0.9871          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:26<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.5s0.2s\n",
      "                   all         51         51       0.73      0.844      0.818      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      34/40         0G     0.5105     0.3958     0.9879          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:22<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.6it/s 2.3s0.2s\n",
      "                   all         51         51      0.774      0.852      0.861      0.735\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      35/40         0G     0.5109     0.3749     0.9945          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:23<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.1it/s 2.5s0.2s\n",
      "                   all         51         51      0.775      0.788      0.818      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      36/40         0G     0.4853     0.3691     0.9773          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:27<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.7it/s 2.3s0.2s\n",
      "                   all         51         51      0.776      0.841      0.849      0.698\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      37/40         0G     0.4835     0.3593      0.974          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:23<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.5it/s 2.4s0.2s\n",
      "                   all         51         51      0.739      0.848      0.826       0.68\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      38/40         0G     0.4683     0.3556     0.9632          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:26<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.3it/s 2.4s0.2s\n",
      "                   all         51         51      0.744      0.874      0.818      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      39/40         0G     0.4585     0.3401     0.9736          1        416: 100% ━━━━━━━━━━━━ 925/925 3.4it/s 4:31<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.769      0.861      0.837      0.714\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      40/40         0G     0.4465     0.3424     0.9612          1        416: 100% ━━━━━━━━━━━━ 925/925 3.5it/s 4:26<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.0it/s 2.6s0.2s\n",
      "                   all         51         51      0.789      0.817      0.838      0.709\n",
      "\n",
      "40 epochs completed in 2.940 hours.\n",
      "Optimizer stripped from D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs\\train_exp\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs\\train_exp\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs\\train_exp\\weights\\best.pt...\n",
      "Ultralytics 8.3.210  Python-3.13.9 torch-2.8.0+cu126 CPU (Intel Core i9-14900HX)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 5.4it/s 2.4s0.2s\n",
      "                   all         51         51      0.774      0.852      0.861      0.735\n",
      "             Capacitor         27         27      0.851      0.849      0.882      0.748\n",
      "                    IC          7          7      0.746      0.843      0.693      0.382\n",
      "             Processor         10         10      0.781      0.715      0.874      0.828\n",
      "               Tan_Cap          7          7      0.718          1      0.995      0.981\n",
      "Speed: 0.5ms preprocess, 36.3ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs\\train_exp\u001b[0m\n",
      "Best model: D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\runs\\train_exp\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "best = train_yolo()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "349c4cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.210  Python-3.13.9 torch-2.8.0+cu126 CPU (Intel Core i9-14900HX)\n",
      "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 914.0889.3 MB/s, size: 61.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Coding\\Projects\\Classifier\\Waste_Classifier\\yolo_data\\labels\\val.cache... 51 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 51/51 127.3Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s0.9s\n",
      "                   all         51         51      0.762      0.856      0.858      0.726\n",
      "             Capacitor         27         27      0.791      0.852      0.869       0.74\n",
      "                    IC          7          7      0.749      0.853      0.696      0.384\n",
      "             Processor         10         10      0.782      0.718      0.873      0.817\n",
      "               Tan_Cap          7          7      0.724          1      0.995      0.963\n",
      "Speed: 0.5ms preprocess, 30.0ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "mAP50-95: 0.7260 | mAP50: 0.8583\n",
      "Capacitor: AP50-95=0.7401\n",
      "IC: AP50-95=0.3835\n",
      "Processor: AP50-95=0.8174\n",
      "Tan_Cap: AP50-95=0.9628\n"
     ]
    }
   ],
   "source": [
    "validate(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8138287",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_video_classify(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4605bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
